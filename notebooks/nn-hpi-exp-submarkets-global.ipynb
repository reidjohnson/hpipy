{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564e860e-d1d8-4af8-aa59-502bbe757a2e",
   "metadata": {},
   "source": [
    "## NN HPI Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a64c60-882c-4398-9759-2a25afd778f6",
   "metadata": {},
   "source": [
    "Generates neural net HPIs on King County data and saves them to CSV files. Requires hpiPy to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12856c18-aece-4176-b55d-ba51a2b6f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, sys\n",
    "sys.path.insert(1, str(pathlib.Path().resolve().parents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be52f9a3-2438-44ea-b251-e10c4f51b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a6de27-7864-47e7-adac-beaed9340683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('mimetype')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hpipy.extensions import NeuralNetworkIndex, RandomForestIndex\n",
    "from hpipy.period_table import PeriodTable\n",
    "from hpipy.price_index import HedonicIndex, RepeatTransactionIndex\n",
    "from hpipy.trans_data import HedonicTransactionData, RepeatTransactionData\n",
    "from hpipy.utils.metrics import (\n",
    "    accuracy,\n",
    "    revision,\n",
    "    series_accuracy,\n",
    "    series_volatility,\n",
    "    volatility,\n",
    ")\n",
    "from hpipy.utils.plotting import (\n",
    "    plot_index,\n",
    "    plot_index_accuracy,\n",
    "    plot_index_volatility,\n",
    "    plot_series_revision,\n",
    "    plot_series_volatility,\n",
    ")\n",
    "\n",
    "alt.renderers.enable(\"mimetype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1caf324-eb13-4cee-8537-3b850adf1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv('../data/kingco_sales.csv', index_col=0, parse_dates=['sale_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fd0b0-f4a0-4348-b0b1-1da482b4f40a",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62033144-9712-431e-80ff-9090a5060a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_kc_data(df_sales):\n",
    "    df = df_sales.copy()\n",
    "\n",
    "    # Remove sales with 'bad' status.\n",
    "    allowed_status = [\"new\", \"nochg\", \"rebuilt - before\", \"reno - before\"]\n",
    "    df = df[df[\"join_status\"].isin(allowed_status)].copy()\n",
    "\n",
    "    df[\"baths\"] = df[\"bath_full\"] + (df[\"bath_3qtr\"] * 0.75) + (df[\"bath_half\"] * 0.5)\n",
    "    df[\"garage_sqft\"] = df[\"garb_sqft\"] + df[\"gara_sqft\"]\n",
    "    df[\"view_score\"] = (\n",
    "        df[\"view_rainier\"]\n",
    "        + df[\"view_olympics\"]\n",
    "        + df[\"view_cascades\"]\n",
    "        + df[\"view_sound\"]\n",
    "        + df[\"view_lakewash\"]\n",
    "    )\n",
    "    df[\"tax_value\"] = df[\"land_val\"] + df[\"imp_val\"]\n",
    "\n",
    "    def waterfront_type(row):\n",
    "        \"\"\"Waterfront type.\"\"\"\n",
    "        if row[\"wfnt\"] in [4, 5, 8]:\n",
    "            return \"lake\"\n",
    "        elif row[\"wfnt\"] in [1, 9]:\n",
    "            return \"river\"\n",
    "        elif row[\"wfnt\"] in [2, 3]:\n",
    "            return \"puget_sound\"\n",
    "        elif row[\"wfnt\"] == 6:\n",
    "            return \"lake_wash\"\n",
    "        elif row[\"wfnt\"] == 7:\n",
    "            return \"lake_samm\"\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    df[\"waterfront_type\"] = df.apply(waterfront_type, axis=1)\n",
    "    df[\"is_wfnt\"] = df[\"wfnt\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "    df[\"age\"] = (df[\"sale_date\"].dt.year - df[\"year_built\"]).clip(lower=0)\n",
    "\n",
    "    def effective_age(row):\n",
    "        if row[\"year_reno\"] == 0:\n",
    "            return row[\"age\"]\n",
    "        else:\n",
    "            return row[\"sale_date\"].year - row[\"year_reno\"]\n",
    "\n",
    "    df[\"eff_age\"] = df.apply(effective_age, axis=1)\n",
    "    df[\"green_adjacent\"] = df.apply(\n",
    "        lambda row: 1 if row[\"golf\"] == 1 or row[\"greenbelt\"] == 1 else 0, axis=1\n",
    "    )\n",
    "    df[\"townhome\"] = df[\"present_use\"].apply(lambda x: 1 if x == 29 else 0)\n",
    "\n",
    "    columns_to_select = [\n",
    "        \"sale_id\",\n",
    "        \"pinx\",\n",
    "        \"sale_date\",\n",
    "        \"sale_price\",\n",
    "        \"join_status\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"city\",\n",
    "        \"area\",\n",
    "        \"submarket\",\n",
    "        \"subdivision\",\n",
    "        \"present_use\",\n",
    "        \"land_val\",\n",
    "        \"imp_val\",\n",
    "        \"year_built\",\n",
    "        \"year_reno\",\n",
    "        \"age\",\n",
    "        \"eff_age\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft\",\n",
    "        \"grade\",\n",
    "        \"condition\",\n",
    "        \"stories\",\n",
    "        \"beds\",\n",
    "        \"baths\",\n",
    "        \"garb_sqft\",\n",
    "        \"gara_sqft\",\n",
    "        \"waterfront_type\",\n",
    "        \"is_wfnt\",\n",
    "        \"golf\",\n",
    "        \"greenbelt\",\n",
    "        \"noise_traffic\",\n",
    "        \"view_score\",\n",
    "    ]\n",
    "    df = df[columns_to_select]\n",
    "\n",
    "    # Fix Vashon Island submarket.\n",
    "    df[\"submarket\"] = df.apply(lambda x: \"I\" if x[\"submarket\"] == \"H\" else x[\"submarket\"], axis=\"columns\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = prep_kc_data(df_sales)\n",
    "df.to_csv(\"../data/king_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52ae4d-b52a-4b00-a906-ca27c561fdc8",
   "metadata": {},
   "source": [
    "## Prepare Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1071ce-c97a-4db4-bdca-aa458a2f7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Experiment Object for time = 5\n",
      "Building Experiment Object for time = 10\n",
      "Building Experiment Object for time = 20\n"
     ]
    }
   ],
   "source": [
    "def prep_kc_exp():\n",
    "    \"\"\"Create standard experiments (those with different time coverage).\"\"\"\n",
    "    sales_df = pd.read_csv(\"../data/king_df.csv\", parse_dates=[\"sale_date\"])\n",
    "\n",
    "    # Set experiment parameters.\n",
    "    terminal_year = 2024\n",
    "    time_ranges = [5, 10, 20]\n",
    "    sms = [\"county\", \"submarket\"]\n",
    "    periodicity = \"monthly\"\n",
    "    train_per = 0.2\n",
    "\n",
    "    # For HED/RF.\n",
    "    ind_var = [\n",
    "        \"age\",\n",
    "        \"baths\",\n",
    "        \"beds\",\n",
    "        \"grade\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"present_use\",\n",
    "        \"sqft\",\n",
    "        \"sqft_lot\",\n",
    "        \"submarket\",\n",
    "    ]\n",
    "\n",
    "    # For RF only.\n",
    "    rf_par = {\n",
    "        \"num_estimators\": 200,\n",
    "        \"sim_per\": 0.10,\n",
    "        \"min_bucket\": 5,\n",
    "    }\n",
    "\n",
    "    nn_par = {\n",
    "        \"preprocess_geo\": True,\n",
    "        \"feature_dict\": {\n",
    "            \"numerics\": [],\n",
    "            \"log_numerics\": [\"age\", \"sqft\", \"sqft_lot\"],\n",
    "            \"categoricals\": [\"present_use\", \"submarket\"],\n",
    "            \"ordinals\": [\"baths\", \"beds\", \"grade\"],\n",
    "            \"hpi\": [\"sale_date\", \"submarket\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def calculate_train_period(tr, periodicity, train_per):\n",
    "        \"\"\"Calculate train period based on periodicity.\"\"\"\n",
    "        if periodicity == \"monthly\":\n",
    "            return int(round(tr * 12 * train_per, 0))\n",
    "        else:\n",
    "            return int(round(tr * 52 * train_per, 0))\n",
    "\n",
    "    # For each desired time range, build data.\n",
    "    for tr in time_ranges:\n",
    "        print(f\"Building Experiment Object for time = {tr}\")\n",
    "\n",
    "        # Set up Experiment Control Object.\n",
    "        exp_obj = {\n",
    "            \"name\": f\"exp_{tr}\",\n",
    "            \"time\": tr,\n",
    "            \"start_date\": f\"{terminal_year - tr}-01-01\",\n",
    "            \"sms\": sms,\n",
    "            \"periodicity\": periodicity,\n",
    "            \"train_period\": calculate_train_period(tr, periodicity, train_per),\n",
    "            \"ind_var\": ind_var,\n",
    "            \"rf_par\": rf_par,\n",
    "            \"nn_par\": nn_par,\n",
    "        }\n",
    "\n",
    "        # Check for directory, create if not present.\n",
    "        exp_dir = os.path.join(\"data\", exp_obj[\"name\"])\n",
    "        if not os.path.exists(exp_dir):\n",
    "            os.makedirs(exp_dir)\n",
    "\n",
    "        #exp_obj[\"rt_df\"] = RepeatTransactionData(\n",
    "        #    sales_df[sales_df[\"sale_date\"] >= exp_obj[\"start_date\"]].copy()\n",
    "        #).create_transactions(\n",
    "        #    prop_id=\"pinx\",\n",
    "        #    trans_id=\"sale_id\",\n",
    "        #    price=\"sale_price\",\n",
    "        #    date=\"sale_date\",\n",
    "        #    periodicity=exp_obj[\"periodicity\"],\n",
    "        #    seq_only=True,\n",
    "        #    min_period_dist=exp_obj[\"train_period\"],\n",
    "        #)\n",
    "\n",
    "        exp_obj[\"hed_df\"] = HedonicTransactionData(\n",
    "            sales_df[sales_df[\"sale_date\"] >= exp_obj[\"start_date\"]].copy()\n",
    "        ).create_transactions(\n",
    "            prop_id=\"pinx\",\n",
    "            trans_id=\"sale_id\",\n",
    "            price=\"sale_price\",\n",
    "            date=\"sale_date\",\n",
    "            periodicity=exp_obj[\"periodicity\"],\n",
    "        )\n",
    "\n",
    "        # Write data.\n",
    "        #exp_obj[\"rt_df\"].trans_df.to_csv(\n",
    "        #    os.path.join(exp_dir, \"rt_df.csv\"), index=False\n",
    "        #)\n",
    "        exp_obj[\"hed_df\"].trans_df.to_csv(\n",
    "            os.path.join(exp_dir, \"hed_df.csv\"), index=False\n",
    "        )\n",
    "\n",
    "        # Save the experiment object.\n",
    "        with open(os.path.join(exp_dir, \"exp_obj.pkl\"), \"wb\") as file:\n",
    "            pickle.dump(exp_obj, file)\n",
    "\n",
    "\n",
    "prep_kc_exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c4532-de15-4c91-bd45-b9bc6916e89a",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c16fdd-de2f-4980-a1f8-46913a5af881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ nn --- exp_10 --- all -----------------\n",
      "Running series (24 to 120)...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def exp_wrapper(\n",
    "    exp_obj,\n",
    "    partition=\"all\",\n",
    "    verbose=False,\n",
    "    estimator=\"robust\",\n",
    "    log_dep=True,\n",
    "    vol_window=3,\n",
    "    index_only=False,\n",
    "):\n",
    "    \"\"\"Experiment wrapper.\"\"\"\n",
    "    # Estimate the indices.\n",
    "    if verbose:\n",
    "        print(f\"Estimating index with {exp_obj['model']} model...\\n\")\n",
    "\n",
    "    if exp_obj[\"model\"] == \"rt\":\n",
    "        hpi_obj = RepeatTransactionIndex.create_index(\n",
    "            trans_data=exp_obj[\"rt_df\"],\n",
    "            estimator=estimator,\n",
    "            log_dep=log_dep,\n",
    "            max_period=max(exp_obj[\"rt_df\"].trans_df[\"period_2\"]),\n",
    "        )\n",
    "\n",
    "    if exp_obj[\"model\"] == \"hed\":\n",
    "        hpi_obj = HedonicIndex.create_index(\n",
    "            trans_data=exp_obj[\"hed_df\"],\n",
    "            estimator=\"base\",\n",
    "            log_dep=log_dep,\n",
    "            dep_var=\"price\",\n",
    "            ind_var=exp_obj[\"ind_var\"],\n",
    "            max_period=max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"]),\n",
    "            smooth=True,\n",
    "        )\n",
    "\n",
    "    if exp_obj[\"model\"] == \"rf\":\n",
    "        hpi_obj = RandomForestIndex.create_index(\n",
    "            trans_data=exp_obj[\"hed_df\"],\n",
    "            estimator=\"pdp\",\n",
    "            log_dep=log_dep,\n",
    "            dep_var=\"price\",\n",
    "            ind_var=exp_obj[\"ind_var\"],\n",
    "            num_estimators=exp_obj[\"rf_par\"][\"num_estimators\"],\n",
    "            max_period=max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"]),\n",
    "            smooth=True,\n",
    "        )\n",
    "\n",
    "    if exp_obj[\"model\"] == \"nn\":\n",
    "        hpi_obj = NeuralNetworkIndex.create_index(\n",
    "            trans_data=exp_obj[\"hed_df\"],\n",
    "            date=\"sale_date\",\n",
    "            # estimator='attributional',\n",
    "            estimator=\"residual\",\n",
    "            dep_var=\"price\",\n",
    "            ind_var=exp_obj[\"ind_var\"],\n",
    "            max_period=max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"]),\n",
    "            smooth=False,\n",
    "            preprocess_geo=exp_obj[\"nn_par\"][\"preprocess_geo\"],\n",
    "            feature_dict=exp_obj[\"nn_par\"][\"feature_dict\"],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    if isinstance(hpi_obj.value, list):\n",
    "        partition_cols = [col for col in hpi_obj.value[0].columns if col not in [\"index\"]]\n",
    "        periods = np.tile(np.array(hpi_obj.periods), len(hpi_obj.value)).tolist()\n",
    "        value = (\n",
    "            pd.concat(hpi_obj.value)[\"index\"]\n",
    "            if len(hpi_obj.value) > 1\n",
    "            else hpi_obj.value[0][\"index\"]\n",
    "        )\n",
    "        imputed = (\n",
    "            np.concatenate(hpi_obj.imputed) if len(hpi_obj.imputed) > 1 else hpi_obj.imputed[0]\n",
    "        )\n",
    "        partition = (\n",
    "            pd.concat(hpi_obj.value)[partition_cols[0]]\n",
    "            if len(hpi_obj.value) > 1\n",
    "            else hpi_obj.value[0][partition_cols[0]]\n",
    "        )\n",
    "        partition = partition.apply(lambda x: f\"{partition_cols[0]}_{x}\")\n",
    "    else:\n",
    "        periods = hpi_obj.periods\n",
    "        value = hpi_obj.value[\"index\"]\n",
    "        imputed = hpi_obj.imputed\n",
    "\n",
    "    # Extract into a data frame.\n",
    "    index_df = pd.DataFrame(\n",
    "        {\n",
    "            \"period\": periods,\n",
    "            \"value\": value,\n",
    "            \"imputed\": imputed,\n",
    "            \"time\": exp_obj[\"time\"],\n",
    "            \"model\": exp_obj[\"model\"],\n",
    "            \"exp\": exp_obj[\"name\"],\n",
    "            \"partition\": partition,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if index_only:\n",
    "        return index_df\n",
    "\n",
    "    if exp_obj[\"model\"] == \"nn\":\n",
    "        train_period = exp_obj[\"train_period\"]\n",
    "        max_period = max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"])\n",
    "        print(f\"Running series ({train_period} to {max_period})...\\n\")\n",
    "        series_obj = hpi_obj.create_series(train_period=train_period, max_period=max_period)\n",
    "\n",
    "        series_df = []\n",
    "        for i, hpi_i_obj in enumerate(series_obj.hpis):\n",
    "            if isinstance(hpi_i_obj.value, list):\n",
    "                partition_cols = [\n",
    "                    col for col in hpi_i_obj.value[0].columns if col not in [\"index\"]\n",
    "                ]\n",
    "                periods = np.tile(np.array(hpi_i_obj.periods), len(hpi_i_obj.value)).tolist()\n",
    "                value = (\n",
    "                    pd.concat(hpi_i_obj.value)[\"index\"]\n",
    "                    if len(hpi_i_obj.value) > 1\n",
    "                    else hpi_i_obj.value[0][\"index\"]\n",
    "                )\n",
    "                imputed = (\n",
    "                    np.concatenate(hpi_i_obj.imputed)\n",
    "                    if len(hpi_i_obj.imputed) > 1\n",
    "                    else hpi_i_obj.imputed[0]\n",
    "                )\n",
    "                partition = (\n",
    "                    pd.concat(hpi_i_obj.value)[partition_cols[0]]\n",
    "                    if len(hpi_i_obj.value) > 1\n",
    "                    else hpi_i_obj.value[0][partition_cols[0]]\n",
    "                )\n",
    "                partition = partition.apply(lambda x: f\"{partition_cols[0]}_{x}\")\n",
    "            else:\n",
    "                periods = hpi_i_obj.periods\n",
    "                value = hpi_i_obj.value[\"index\"]\n",
    "                imputed = hpi_i_obj.imputed\n",
    "\n",
    "            index_i_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"period\": periods,\n",
    "                    \"value\": value,\n",
    "                    \"imputed\": imputed,\n",
    "                    \"time\": exp_obj[\"time\"],\n",
    "                    \"model\": exp_obj[\"model\"],\n",
    "                    \"exp\": exp_obj[\"name\"],\n",
    "                    \"partition\": partition,\n",
    "                    \"train_period\": exp_obj[\"train_period\"] + i,\n",
    "                }\n",
    "            )\n",
    "            series_df.append(index_i_df)\n",
    "        series_df = pd.concat(series_df)\n",
    "    else:\n",
    "        series_df = None\n",
    "\n",
    "    return {\n",
    "        \"index\": index_df,\n",
    "        \"series\": series_df,\n",
    "    }\n",
    "\n",
    "\n",
    "def read_experiment_object(exp_path):\n",
    "    with open(exp_path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def run_experiments(exp_name, models):\n",
    "    \"\"\"Main experiment function.\"\"\"\n",
    "    exp_dir = os.path.join(\"data\", exp_name)\n",
    "    exp_obj_path = os.path.join(exp_dir, \"exp_obj.pkl\")\n",
    "\n",
    "    exp_obj = read_experiment_object(exp_obj_path)\n",
    "\n",
    "    for model in models:\n",
    "        # Full County\n",
    "        print(f\"------ {model} --- {exp_name} --- all -----------------\")\n",
    "        exp_obj[\"model\"] = model\n",
    "        exp_obj[\"partition\"] = \"all\"\n",
    "        results_obj = exp_wrapper(exp_obj, partition=\"all\", index_only=False)\n",
    "\n",
    "        # Saving results.\n",
    "        with open(os.path.join(exp_dir, f\"{model}_all_global_results.pkl\"), \"wb\") as file:\n",
    "            pickle.dump(results_obj, file)\n",
    "\n",
    "\n",
    "# Define experiment parameters.\n",
    "# time_frames = ['exp_5', 'exp_10', 'exp_20']\n",
    "time_frames = [\"exp_10\"]\n",
    "# models = ['rt', 'hed', 'rf', 'nn']\n",
    "models = [\"nn\"]\n",
    "\n",
    "# Run experiments for each time frame.\n",
    "for exp in time_frames:\n",
    "    run_experiments(exp, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5615a9-1371-4fc7-8dc7-8abb45873fe8",
   "metadata": {},
   "source": [
    "## Save Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae2fb14-b2d9-42af-a669-7bf3cc3ed4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/exp_10/nn_all_global_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# exps = ['exp_5', 'exp_10', 'exp_20']\n",
    "exps = [\"exp_10\"]\n",
    "\n",
    "indices = []\n",
    "series = []\n",
    "for exp in exps:\n",
    "    exp_dir = os.path.join(\"data\", exp)\n",
    "    resfiles = [f for f in os.listdir(exp_dir) if f.endswith(\"_all_global_results.pkl\")]\n",
    "    for res in resfiles:\n",
    "        print(os.path.join(exp_dir, res))\n",
    "        with open(os.path.join(exp_dir, res), \"rb\") as file:\n",
    "            x = pickle.load(file)\n",
    "        if \"index\" in list(x.keys()):\n",
    "            if x[\"index\"][\"model\"].iloc[0] == \"nn\":\n",
    "                indices.append(x[\"index\"])\n",
    "                series.append(x[\"series\"])\n",
    "        elif x[\"model\"].iloc[0] == \"nn\":\n",
    "            indices.append(x)\n",
    "df_indices = pd.concat(indices)\n",
    "df_indices.to_csv(\"exp_nn_submarkets_global_results_py.csv\", index=False)\n",
    "if series:\n",
    "    df_series = pd.concat(series)\n",
    "    df_series.to_csv(\"exp_nn_series_submarkets_global_results_py.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
