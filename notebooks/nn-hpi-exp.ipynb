{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564e860e-d1d8-4af8-aa59-502bbe757a2e",
   "metadata": {},
   "source": [
    "## NN HPI Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a64c60-882c-4398-9759-2a25afd778f6",
   "metadata": {},
   "source": [
    "Generates neural net HPIs on King County data and saves them to CSV files. Requires hpiPy to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12856c18-aece-4176-b55d-ba51a2b6f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, sys\n",
    "\n",
    "sys.path.insert(1, str(pathlib.Path().resolve().parents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52f9a3-2438-44ea-b251-e10c4f51b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a6de27-7864-47e7-adac-beaed9340683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('mimetype')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hpipy.extensions import NeuralNetworkIndex, RandomForestIndex\n",
    "from hpipy.period_table import PeriodTable\n",
    "from hpipy.price_index import HedonicIndex, RepeatTransactionIndex\n",
    "from hpipy.trans_data import HedonicTransactionData, RepeatTransactionData\n",
    "from hpipy.utils.metrics import (\n",
    "    accuracy,\n",
    "    revision,\n",
    "    series_accuracy,\n",
    "    series_volatility,\n",
    "    volatility,\n",
    ")\n",
    "from hpipy.utils.plotting import (\n",
    "    plot_index,\n",
    "    plot_index_accuracy,\n",
    "    plot_index_volatility,\n",
    "    plot_series_revision,\n",
    "    plot_series_volatility,\n",
    ")\n",
    "\n",
    "alt.renderers.enable(\"mimetype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1caf324-eb13-4cee-8537-3b850adf1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv(\"../data/kingco_sales.csv\", index_col=0, parse_dates=[\"sale_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fd0b0-f4a0-4348-b0b1-1da482b4f40a",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62033144-9712-431e-80ff-9090a5060a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_kc_data(df_sales):\n",
    "    df = df_sales.copy()\n",
    "\n",
    "    # Remove sales with 'bad' status.\n",
    "    allowed_status = [\"new\", \"nochg\", \"rebuilt - before\", \"reno - before\"]\n",
    "    df = df[df[\"join_status\"].isin(allowed_status)].copy()\n",
    "\n",
    "    df[\"baths\"] = df[\"bath_full\"] + (df[\"bath_3qtr\"] * 0.75) + (df[\"bath_half\"] * 0.5)\n",
    "    df[\"garage_sqft\"] = df[\"garb_sqft\"] + df[\"gara_sqft\"]\n",
    "    df[\"view_score\"] = (\n",
    "        df[\"view_rainier\"]\n",
    "        + df[\"view_olympics\"]\n",
    "        + df[\"view_cascades\"]\n",
    "        + df[\"view_sound\"]\n",
    "        + df[\"view_lakewash\"]\n",
    "    )\n",
    "    df[\"tax_value\"] = df[\"land_val\"] + df[\"imp_val\"]\n",
    "\n",
    "    def waterfront_type(row):\n",
    "        \"\"\"Waterfront type.\"\"\"\n",
    "        if row[\"wfnt\"] in [4, 5, 8]:\n",
    "            return \"lake\"\n",
    "        elif row[\"wfnt\"] in [1, 9]:\n",
    "            return \"river\"\n",
    "        elif row[\"wfnt\"] in [2, 3]:\n",
    "            return \"puget_sound\"\n",
    "        elif row[\"wfnt\"] == 6:\n",
    "            return \"lake_wash\"\n",
    "        elif row[\"wfnt\"] == 7:\n",
    "            return \"lake_samm\"\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    df[\"waterfront_type\"] = df.apply(waterfront_type, axis=1)\n",
    "    df[\"is_wfnt\"] = df[\"wfnt\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "    df[\"age\"] = (df[\"sale_date\"].dt.year - df[\"year_built\"]).clip(lower=0)\n",
    "\n",
    "    def effective_age(row):\n",
    "        if row[\"year_reno\"] == 0:\n",
    "            return row[\"age\"]\n",
    "        else:\n",
    "            return row[\"sale_date\"].year - row[\"year_reno\"]\n",
    "\n",
    "    df[\"eff_age\"] = df.apply(effective_age, axis=1)\n",
    "    df[\"green_adjacent\"] = df.apply(\n",
    "        lambda row: 1 if row[\"golf\"] == 1 or row[\"greenbelt\"] == 1 else 0, axis=1\n",
    "    )\n",
    "    df[\"townhome\"] = df[\"present_use\"].apply(lambda x: 1 if x == 29 else 0)\n",
    "\n",
    "    columns_to_select = [\n",
    "        \"sale_id\",\n",
    "        \"pinx\",\n",
    "        \"sale_date\",\n",
    "        \"sale_price\",\n",
    "        \"join_status\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"city\",\n",
    "        \"area\",\n",
    "        \"submarket\",\n",
    "        \"subdivision\",\n",
    "        \"present_use\",\n",
    "        \"land_val\",\n",
    "        \"imp_val\",\n",
    "        \"year_built\",\n",
    "        \"year_reno\",\n",
    "        \"age\",\n",
    "        \"eff_age\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft\",\n",
    "        \"grade\",\n",
    "        \"condition\",\n",
    "        \"stories\",\n",
    "        \"beds\",\n",
    "        \"baths\",\n",
    "        \"garb_sqft\",\n",
    "        \"gara_sqft\",\n",
    "        \"waterfront_type\",\n",
    "        \"is_wfnt\",\n",
    "        \"golf\",\n",
    "        \"greenbelt\",\n",
    "        \"noise_traffic\",\n",
    "        \"view_score\",\n",
    "    ]\n",
    "    df = df[columns_to_select]\n",
    "\n",
    "    # Fix Vashon Island submarket.\n",
    "    df[\"submarket\"] = df.apply(\n",
    "        lambda x: \"I\" if x[\"submarket\"] == \"H\" else x[\"submarket\"], axis=\"columns\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = prep_kc_data(df_sales)\n",
    "df.to_csv(\"../data/king_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52ae4d-b52a-4b00-a906-ca27c561fdc8",
   "metadata": {},
   "source": [
    "## Prepare Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1071ce-c97a-4db4-bdca-aa458a2f7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Experiment Object for time = 5\n",
      "Building Experiment Object for time = 10\n",
      "Building Experiment Object for time = 20\n"
     ]
    }
   ],
   "source": [
    "def prep_kc_exp():\n",
    "    \"\"\"Create standard experiments (those with different time coverage).\"\"\"\n",
    "    sales_df = pd.read_csv(\"../data/king_df.csv\", parse_dates=[\"sale_date\"])\n",
    "\n",
    "    # Set experiment parameters.\n",
    "    terminal_year = 2024\n",
    "    time_ranges = [5, 10, 20]\n",
    "    sms = [\"county\", \"submarket\"]\n",
    "    periodicity = \"monthly\"\n",
    "    train_per = 0.2\n",
    "\n",
    "    # For HED/RF.\n",
    "    ind_var = [\n",
    "        \"age\",\n",
    "        \"baths\",\n",
    "        \"beds\",\n",
    "        \"grade\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"present_use\",\n",
    "        \"sqft\",\n",
    "        \"sqft_lot\",\n",
    "        \"submarket\",\n",
    "    ]\n",
    "\n",
    "    # For RF only.\n",
    "    rf_par = {\n",
    "        \"num_estimators\": 200,\n",
    "        \"sim_per\": 0.10,\n",
    "        \"min_bucket\": 5,\n",
    "    }\n",
    "\n",
    "    nn_par = {\n",
    "        \"preprocess_geo\": True,\n",
    "        \"feature_dict\": {\n",
    "            \"numerics\": [],\n",
    "            \"log_numerics\": [\"age\", \"sqft\", \"sqft_lot\"],\n",
    "            \"categoricals\": [\"present_use\", \"submarket\"],\n",
    "            \"ordinals\": [\"baths\", \"beds\", \"grade\"],\n",
    "            \"hpi\": [\"sale_date\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def calculate_train_period(tr, periodicity, train_per):\n",
    "        \"\"\"Calculate train period based on periodicity.\"\"\"\n",
    "        if periodicity == \"monthly\":\n",
    "            return int(round(tr * 12 * train_per, 0))\n",
    "        else:\n",
    "            return int(round(tr * 52 * train_per, 0))\n",
    "\n",
    "    # For each desired time range, build data.\n",
    "    for tr in time_ranges:\n",
    "        print(f\"Building Experiment Object for time = {tr}\")\n",
    "\n",
    "        # Set up Experiment Control Object.\n",
    "        exp_obj = {\n",
    "            \"name\": f\"exp_{tr}\",\n",
    "            \"time\": tr,\n",
    "            \"start_date\": f\"{terminal_year - tr}-01-01\",\n",
    "            \"sms\": sms,\n",
    "            \"periodicity\": periodicity,\n",
    "            \"train_period\": calculate_train_period(tr, periodicity, train_per),\n",
    "            \"ind_var\": ind_var,\n",
    "            \"rf_par\": rf_par,\n",
    "            \"nn_par\": nn_par,\n",
    "        }\n",
    "\n",
    "        # Check for directory, create if not present.\n",
    "        exp_dir = os.path.join(\"data\", exp_obj[\"name\"])\n",
    "        if not os.path.exists(exp_dir):\n",
    "            os.makedirs(exp_dir)\n",
    "\n",
    "        exp_obj[\"rt_df\"] = RepeatTransactionData(\n",
    "            sales_df[sales_df[\"sale_date\"] >= exp_obj[\"start_date\"]].copy()\n",
    "        ).create_transactions(\n",
    "            prop_id=\"pinx\",\n",
    "            trans_id=\"sale_id\",\n",
    "            price=\"sale_price\",\n",
    "            date=\"sale_date\",\n",
    "            periodicity=exp_obj[\"periodicity\"],\n",
    "            seq_only=True,\n",
    "            min_period_dist=exp_obj[\"train_period\"],\n",
    "        )\n",
    "\n",
    "        exp_obj[\"hed_df\"] = HedonicTransactionData(\n",
    "            sales_df[sales_df[\"sale_date\"] >= exp_obj[\"start_date\"]].copy()\n",
    "        ).create_transactions(\n",
    "            prop_id=\"pinx\",\n",
    "            trans_id=\"sale_id\",\n",
    "            price=\"sale_price\",\n",
    "            date=\"sale_date\",\n",
    "            periodicity=exp_obj[\"periodicity\"],\n",
    "        )\n",
    "\n",
    "        # Write data.\n",
    "        exp_obj[\"rt_df\"].trans_df.to_csv(os.path.join(exp_dir, \"rt_df.csv\"), index=False)\n",
    "        exp_obj[\"hed_df\"].trans_df.to_csv(os.path.join(exp_dir, \"hed_df.csv\"), index=False)\n",
    "\n",
    "        # Save the experiment object.\n",
    "        with open(os.path.join(exp_dir, \"exp_obj.pkl\"), \"wb\") as file:\n",
    "            pickle.dump(exp_obj, file)\n",
    "\n",
    "\n",
    "prep_kc_exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c4532-de15-4c91-bd45-b9bc6916e89a",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c16fdd-de2f-4980-a1f8-46913a5af881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ nn --- exp_10 --- all -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_D -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_R -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_I -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_F -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_N -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_S -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_O -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_C -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_Q -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_B -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_J -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_A -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_K -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_P -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_G -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_E -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_M -----------------\n",
      "Running series (24 to 120)...\n",
      "\n",
      "------ nn --- exp_10 --- submarket_L -----------------\n",
      "Running series (24 to 120)...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def exp_wrapper(\n",
    "    exp_obj,\n",
    "    partition=\"all\",\n",
    "    verbose=False,\n",
    "    estimator=\"robust\",\n",
    "    log_dep=True,\n",
    "    vol_window=3,\n",
    "    index_only=False,\n",
    "):\n",
    "    \"\"\"Experiment wrapper.\"\"\"\n",
    "    # Get the max period.\n",
    "    max_period = max(exp_obj[\"rt_df\"].trans_df[\"period_2\"])\n",
    "\n",
    "    # Estimate the indices.\n",
    "    if verbose:\n",
    "        print(f\"Estimating index with {exp_obj['model']} model...\\n\")\n",
    "\n",
    "    if exp_obj[\"model\"] == \"rt\":\n",
    "        hpi_obj = RepeatTransactionIndex.create_index(\n",
    "            trans_data=exp_obj[\"rt_df\"],\n",
    "            estimator=estimator,\n",
    "            log_dep=log_dep,\n",
    "            max_period=max_period,\n",
    "        )\n",
    "\n",
    "    if exp_obj[\"model\"] == \"hed\":\n",
    "        hpi_obj = HedonicIndex.create_index(\n",
    "            trans_data=exp_obj[\"hed_df\"],\n",
    "            estimator=\"base\",\n",
    "            log_dep=log_dep,\n",
    "            dep_var=\"price\",\n",
    "            ind_var=exp_obj[\"ind_var\"],\n",
    "            max_period=max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"]),\n",
    "            smooth=True,\n",
    "        )\n",
    "\n",
    "    if exp_obj[\"model\"] == \"rf\":\n",
    "        hpi_obj = RandomForestIndex.create_index(\n",
    "            trans_data=exp_obj[\"hed_df\"],\n",
    "            estimator=\"pdp\",\n",
    "            log_dep=log_dep,\n",
    "            dep_var=\"price\",\n",
    "            ind_var=exp_obj[\"ind_var\"],\n",
    "            num_estimators=exp_obj[\"rf_par\"][\"num_estimators\"],\n",
    "            max_period=max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"]),\n",
    "            smooth=True,\n",
    "        )\n",
    "\n",
    "    if exp_obj[\"model\"] == \"nn\":\n",
    "        hpi_obj = NeuralNetworkIndex.create_index(\n",
    "            trans_data=exp_obj[\"hed_df\"],\n",
    "            date=\"sale_date\",\n",
    "            # estimator=\"attributional\",\n",
    "            estimator=\"residual\",\n",
    "            dep_var=\"price\",\n",
    "            ind_var=exp_obj[\"ind_var\"],\n",
    "            max_period=max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"]),\n",
    "            smooth=False,\n",
    "            preprocess_geo=exp_obj[\"nn_par\"][\"preprocess_geo\"],\n",
    "            feature_dict=exp_obj[\"nn_par\"][\"feature_dict\"],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    # Extract into a data frame.\n",
    "    index_df = pd.DataFrame(\n",
    "        {\n",
    "            \"period\": hpi_obj.periods,\n",
    "            \"value\": hpi_obj.value,\n",
    "            \"imputed\": hpi_obj.imputed,\n",
    "            \"time\": exp_obj[\"time\"],\n",
    "            \"model\": exp_obj[\"model\"],\n",
    "            \"exp\": exp_obj[\"name\"],\n",
    "            \"partition\": partition,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if index_only:\n",
    "        return index_df\n",
    "\n",
    "    if exp_obj[\"model\"] == \"nn\":\n",
    "        train_period = exp_obj[\"train_period\"]\n",
    "        max_period = max(exp_obj[\"hed_df\"].trans_df[\"trans_period\"])\n",
    "        print(f\"Running series ({train_period} to {max_period})...\\n\")\n",
    "        series_obj = hpi_obj.create_series(train_period=train_period, max_period=max_period)\n",
    "\n",
    "        series_df = []\n",
    "        for i, hpi_i_obj in enumerate(series_obj.hpis):\n",
    "            index_i_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"period\": hpi_i_obj.periods,\n",
    "                    \"value\": hpi_i_obj.value,\n",
    "                    \"imputed\": hpi_i_obj.imputed,\n",
    "                    \"time\": exp_obj[\"time\"],\n",
    "                    \"model\": exp_obj[\"model\"],\n",
    "                    \"exp\": exp_obj[\"name\"],\n",
    "                    \"partition\": partition,\n",
    "                    \"train_period\": exp_obj[\"train_period\"] + i,\n",
    "                }\n",
    "            )\n",
    "            series_df.append(index_i_df)\n",
    "        series_df = pd.concat(series_df)\n",
    "    else:\n",
    "        series_df = None\n",
    "\n",
    "    return {\n",
    "        \"index\": index_df,\n",
    "        \"series\": series_df,\n",
    "    }\n",
    "\n",
    "\n",
    "def read_experiment_object(exp_path):\n",
    "    with open(exp_path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def run_experiments(exp_name, models):\n",
    "    \"\"\"Main experiment function.\"\"\"\n",
    "    exp_dir = os.path.join(\"data\", exp_name)\n",
    "    exp_obj_path = os.path.join(exp_dir, \"exp_obj.pkl\")\n",
    "\n",
    "    exp_obj = read_experiment_object(exp_obj_path)\n",
    "\n",
    "    for model in models:\n",
    "        # Full county.\n",
    "        print(f\"------ {model} --- {exp_name} --- all -----------------\")\n",
    "        exp_obj[\"model\"] = model\n",
    "        exp_obj[\"partition\"] = \"all\"\n",
    "        results_obj = exp_wrapper(exp_obj, partition=\"all\", index_only=False)\n",
    "\n",
    "        # Saving results.\n",
    "        with open(os.path.join(exp_dir, f\"{model}_all_results.pkl\"), \"wb\") as file:\n",
    "            pickle.dump(results_obj, file)\n",
    "\n",
    "        # Submarkets.\n",
    "        exp_obj_s = copy.deepcopy(exp_obj)\n",
    "        exp_obj_s[\"ind_var\"] = [\n",
    "            \"age\",\n",
    "            \"baths\",\n",
    "            \"beds\",\n",
    "            \"grade\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"present_use\",\n",
    "            \"sqft\",\n",
    "            \"sqft_lot\",\n",
    "        ]\n",
    "        for submarket in sorted(df[\"submarket\"].unique()):\n",
    "            print(f\"------ {model} --- {exp_name} --- submarket_{submarket} -----------------\")\n",
    "            exp_obj_i = copy.deepcopy(exp_obj_s)\n",
    "            exp_obj_i[\"model\"] = model\n",
    "            exp_obj_i[\"partition\"] = f\"submarket_{submarket}\"\n",
    "            ss_ids = (\n",
    "                exp_obj_i[\"hed_df\"]\n",
    "                .trans_df.query(f\"submarket == '{submarket}'\")[\"trans_id\"]\n",
    "                .to_list()\n",
    "            )\n",
    "            exp_obj_i[\"hed_df\"].trans_df = exp_obj_i[\"hed_df\"].trans_df.query(\n",
    "                f\"trans_id in {ss_ids}\"\n",
    "            )\n",
    "            for dtype in exp_obj[\"nn_par\"][\"feature_dict\"].keys():\n",
    "                exp_obj_i[\"nn_par\"][\"feature_dict\"][dtype] = list(\n",
    "                    set(exp_obj_i[\"nn_par\"][\"feature_dict\"][dtype]).intersection(\n",
    "                        exp_obj_i[\"ind_var\"] + [\"sale_date\"]\n",
    "                    )\n",
    "                )\n",
    "            results_obj = exp_wrapper(\n",
    "                exp_obj_i, partition=f\"submarket_{submarket}\", index_only=False\n",
    "            )\n",
    "            with open(\n",
    "                os.path.join(exp_dir, f\"{model}_submarket_{submarket}_local_results.pkl\"), \"wb\"\n",
    "            ) as file:\n",
    "                pickle.dump(results_obj, file)\n",
    "\n",
    "\n",
    "# Define experiment parameters.\n",
    "# time_frames = [\"exp_5\", \"exp_10\", \"exp_20\"]\n",
    "time_frames = [\"exp_10\"]\n",
    "# models = [\"rt\", \"hed\", \"rf\", \"nn\"]\n",
    "models = [\"nn\"]\n",
    "\n",
    "# Run experiments for each time frame.\n",
    "for exp in time_frames:\n",
    "    run_experiments(exp, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5615a9-1371-4fc7-8dc7-8abb45873fe8",
   "metadata": {},
   "source": [
    "## Save Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b014b7c6-9855-41f1-abd1-b69051323d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/exp_10/nn_all_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# exps = [\"exp_5\", \"exp_10\", \"exp_20\"]\n",
    "exps = [\"exp_10\"]\n",
    "\n",
    "indices = []\n",
    "series = []\n",
    "for exp in exps:\n",
    "    exp_dir = os.path.join(\"data\", exp)\n",
    "    resfiles = [f for f in os.listdir(exp_dir) if f.endswith(\"_all_results.pkl\")]\n",
    "    for res in resfiles:\n",
    "        print(os.path.join(exp_dir, res))\n",
    "        with open(os.path.join(exp_dir, res), \"rb\") as file:\n",
    "            x = pickle.load(file)\n",
    "        if \"index\" in list(x.keys()):\n",
    "            if x[\"index\"][\"model\"].iloc[0] == \"nn\":\n",
    "                indices.append(x[\"index\"])\n",
    "                series.append(x[\"series\"])\n",
    "        elif x[\"model\"].iloc[0] == \"nn\":\n",
    "            indices.append(x)\n",
    "df_indices = pd.concat(indices)\n",
    "df_indices.to_csv(\"exp_nn_results_py.csv\", index=False)\n",
    "if series:\n",
    "    df_series = pd.concat(series)\n",
    "    df_series.to_csv(\"exp_nn_series_results_py.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae2fb14-b2d9-42af-a669-7bf3cc3ed4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/exp_10/nn_submarket_A_local_results.pkl\n",
      "data/exp_10/nn_submarket_D_local_results.pkl\n",
      "data/exp_10/nn_submarket_E_local_results.pkl\n",
      "data/exp_10/nn_submarket_C_local_results.pkl\n",
      "data/exp_10/nn_submarket_F_local_results.pkl\n",
      "data/exp_10/nn_submarket_B_local_results.pkl\n",
      "data/exp_10/nn_submarket_G_local_results.pkl\n",
      "data/exp_10/nn_submarket_P_local_results.pkl\n",
      "data/exp_10/nn_submarket_N_local_results.pkl\n",
      "data/exp_10/nn_submarket_K_local_results.pkl\n",
      "data/exp_10/nn_submarket_Q_local_results.pkl\n",
      "data/exp_10/nn_submarket_O_local_results.pkl\n",
      "data/exp_10/nn_submarket_J_local_results.pkl\n",
      "data/exp_10/nn_submarket_R_local_results.pkl\n",
      "data/exp_10/nn_submarket_L_local_results.pkl\n",
      "data/exp_10/nn_submarket_I_local_results.pkl\n",
      "data/exp_10/nn_submarket_S_local_results.pkl\n",
      "data/exp_10/nn_submarket_M_local_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# exps = [\"exp_5\", \"exp_10\", \"exp_20\"]\n",
    "exps = [\"exp_10\"]\n",
    "\n",
    "indices = []\n",
    "series = []\n",
    "for exp in exps:\n",
    "    exp_dir = os.path.join(\"data\", exp)\n",
    "    resfiles = [f for f in os.listdir(exp_dir) if f.endswith(\"_local_results.pkl\")]\n",
    "    for res in resfiles:\n",
    "        print(os.path.join(exp_dir, res))\n",
    "        with open(os.path.join(exp_dir, res), \"rb\") as file:\n",
    "            x = pickle.load(file)\n",
    "        if \"index\" in list(x.keys()):\n",
    "            if x[\"index\"][\"model\"].iloc[0] == \"nn\":\n",
    "                indices.append(x[\"index\"])\n",
    "                series.append(x[\"series\"])\n",
    "        elif x[\"model\"].iloc[0] == \"nn\":\n",
    "            indices.append(x)\n",
    "df_indices = pd.concat(indices)\n",
    "df_indices.to_csv(\"exp_nn_submarkets_local_results_py.csv\", index=False)\n",
    "if series:\n",
    "    df_series = pd.concat(series)\n",
    "    df_series.to_csv(\"exp_nn_series_submarkets_local_results_py.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
